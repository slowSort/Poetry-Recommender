{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 6)\n",
      "(94, 6)\n"
     ]
    }
   ],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "# Import get user profile\n",
    "from userProfile import get_user_profile\n",
    "# Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "\n",
    "\"\"\"\n",
    "Since you have used the TF-IDF vectorizer, calculating the dot product will\n",
    "directly give you the cosine similarity score. Therefore, you will use\n",
    "sklearn's linear_kernel() instead of cosine_similarities() since it is faster.\n",
    "\"\"\"\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"cKeecl00\",\n",
    "  database=\"sys\"\n",
    ")\n",
    "\n",
    "# Load Movies Metadata as a DataFrame\n",
    "poems = pd.read_sql(\"SELECT * FROM poems\", mydb)\n",
    "poems = poems[['poem_id', 'title', 'author',\n",
    "               'lines', 'linecount', 'wordcount']]\n",
    "\n",
    "userProfile = get_user_profile()\n",
    "print(poems.shape)\n",
    "poems = poems.append(userProfile, ignore_index=True)\n",
    "print(poems.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': ['Percy Bysshe Shelley',\n",
       "  'George Gordon, Lord Byron',\n",
       "  'Percy Bysshe Shelley',\n",
       "  'Percy Bysshe Shelley',\n",
       "  'Percy Bysshe Shelley',\n",
       "  'Percy Bysshe Shelley',\n",
       "  'William Blake',\n",
       "  'George Gordon, Lord Byron',\n",
       "  'George Gordon, Lord Byron',\n",
       "  'George Gordon, Lord Byron',\n",
       "  'Percy Bysshe Shelley',\n",
       "  'Percy Bysshe Shelley',\n",
       "  'Percy Bysshe Shelley',\n",
       "  'George Gordon, Lord Byron',\n",
       "  'Percy Bysshe Shelley'],\n",
       " 'linecount': 3.7333333333333334,\n",
       " 'lines': ['[\"His face was like a snake\\'s--wrinkled and loose\", \"And withered--\"]',\n",
       "  '[\"God maddens him whom\\'t is his will to lose,\", \"And gives the choice of death or phrenzy--choose.\"]',\n",
       "  '[\"Rome has fallen, ye see it lying\", \"Heaped in undistinguished ruin:\", \"Nature is alone undying.\"]',\n",
       "  '[\"I went into the deserts of dim sleep--\", \"That world which, like an unknown wilderness,\", \"Bounds this with its recesses wide and deep--\"]',\n",
       "  '[\"Hark! the owlet flaps his wings\", \"In the pathless dell beneath;\", \"Hark! \\'tis the night-raven sings\", \"Tidings of approaching death.\"]',\n",
       "  '[\"Inter marmoreas Leonorae pendula colles\", \"Fortunata nimis Machina dicit horas.\", \"Quas MANIBUS premit illa duas insensa papillas\", \"Cur mihi sit DIGITO tangere, amata, nefas?\"]',\n",
       "  '[\"Does the Eagle know what is in the pit?\", \"Or wilt thou go ask the Mole:\", \"Can Wisdom be put in a silver rod?\", \"Or Love in a golden bowl?\"]',\n",
       "  '[\"\\\\\"Away, away,--your flattering arts\", \"May now betray some simpler hearts;\", \"And _you_ will _smile_ at their believing,\", \"And _they_ shall _weep_ at your deceiving.\\\\\"\"]',\n",
       "  '[\"WITH Death doomed to grapple,\", \"  Beneath this cold slab, he\", \"Who lied in the Chapel\", \"  Now lies in the Abbey.\"]',\n",
       "  '[\"THROUGH Life\\'s dull road, so dim and dirty,\", \"I have dragged to three-and-thirty.\", \"What have these years left to me?\", \"Nothing--except thirty-three.\"]',\n",
       "  '[\"The fierce beasts of the woods and wildernesses\", \"Track not the steps of him who drinks of it;\", \"For the light breezes, which for ever fleet\", \"Around its margin, heap the sand thereon.\"]',\n",
       "  '[\"And where is truth? On tombs? for such to thee\", \"Has been my heart--and thy dead memory\", \"Has lain from childhood, many a changeful year,\", \"Unchangingly preserved and buried there.\"]',\n",
       "  '[\"Great Spirit whom the sea of boundless thought\", \"Nurtures within its unimagined caves,\", \"In which thou sittest sole, as in my mind,\", \"Giving a voice to its mysterious waves--\"]',\n",
       "  '[\"Animula! vagula, Blandula,\", \"Hospes, comesque corporis,\", \"Qu√¶ nunc abibis in Loca--\", \"Pallidula, rigida, nudula,\", \"Nec, ut soles, dabis Jocos?\"]',\n",
       "  '[\"The death knell is ringing\", \"The raven is singing\", \"The earth worm is creeping\", \"The mourners are weeping\", \"Ding dong, bell--\"]'],\n",
       " 'poem_id': 1,\n",
       " 'title': 'USER_PROFILE',\n",
       " 'wordcount': 22.733333333333334}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems['lines'] = poems['lines'].map(lambda x: ''.join(x))\n",
    "\n",
    "# Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "# flatten poem lines into a list of strings\n",
    "\n",
    "# Replace NaN with an empty string\n",
    "poems['lines'] = poems['lines'].fillna('')\n",
    "# Construct the required TF-IDF matrix by fitting and transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf.fit_transform(poems['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity matrix\n",
    "# Linear kernel used as it's faster\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(poems.index, index=poems['poem_id']).drop_duplicates()\n",
    "\n",
    "\n",
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(poem_id, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[poem_id]\n",
    "    # Get the pairwsie similarity scores of all movies with that moviee\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return poems['title'].iloc[movie_indices]\n",
    "\n",
    "\n",
    "def to_similarity_matrix(feature):\n",
    "    'input is an array of numbers, returns a numpy 2d matrix'\n",
    "    simMatrix = []\n",
    "    for x in feature:\n",
    "        row = []\n",
    "        for y in feature:\n",
    "            # this math is quite important\n",
    "            # find difference between two numbers\n",
    "            # add 1 to prevent / 0 error\n",
    "            # divide 1 by new number to get scaled similarity\n",
    "            row.append(1/((max(x, y)-min(x, y))+1))\n",
    "        simMatrix.append(row)\n",
    "    return np.array(simMatrix)\n",
    "\n",
    "\n",
    "# Function to convert all strings to lower case and strip names of spaces\n",
    "# Hyphens separate authors, and so are replaced with spaces for many authors\n",
    "def clean_author(x):\n",
    "    if isinstance(x, list):\n",
    "        return \" \".join(x)\n",
    "    else:\n",
    "        return str.lower(x.replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "poems['author'] = poems['author'].apply(clean_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['percybyssheshelley',\n",
       " 'georgegordon,lordbyron',\n",
       " 'percybyssheshelley',\n",
       " 'percybyssheshelley',\n",
       " 'percybyssheshelley',\n",
       " 'percybyssheshelley',\n",
       " 'williamblake',\n",
       " 'georgegordon,lordbyron',\n",
       " 'georgegordon,lordbyron',\n",
       " 'georgegordon,lordbyron',\n",
       " 'percybyssheshelley',\n",
       " 'percybyssheshelley',\n",
       " 'percybyssheshelley',\n",
       " 'georgegordon,lordbyron',\n",
       " 'percybyssheshelley']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems['author'][93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9f9fdb6baef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    326\u001b[0m                                                tokenize)\n\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 328\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(poems['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5a840c17e0f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauthor_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Downweight author significance in vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mauthor_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    326\u001b[0m                                                tokenize)\n\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 328\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "author_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "# Downweight author significance in vectorizer\n",
    "author_sim = np.multiply(author_sim, 0.1)\n",
    "\n",
    "poems = poems.reset_index()\n",
    "indices = pd.Series(poems.index, index=poems['poem_id']).drop_duplicates()\n",
    "\n",
    "# Create sim matrix for line count\n",
    "lines_sim = to_similarity_matrix(poems['linecount'])\n",
    "# Create sim matrix for word count\n",
    "wordcount_sim = to_similarity_matrix(poems['wordcount'])\n",
    "wordcount_sim = np.multiply(wordcount_sim, 0.2)\n",
    "# Average the two cosine similarities\n",
    "final_features = [cosine_sim, author_sim, lines_sim, wordcount_sim]\n",
    "final_sim = np.mean(np.array(final_features), axis=0)\n",
    "\n",
    "# print(cosine_sim[0])\n",
    "# print(author_sim[0])\n",
    "# print(lines_sim[0])\n",
    "# print(wordcount_sim[0])\n",
    "# print(final_sim[0])\n",
    "\n",
    "# print(get_recommendations(102, cosine_sim))\n",
    "# print(get_recommendations(102, author_sim))\n",
    "# print(get_recommendations(107, lines_sim))\n",
    "# print(get_recommendations(107, wordcount_sim))\n",
    "# print(get_recommendations(102, final_sim))\n",
    "print(get_recommendations(1, final_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
